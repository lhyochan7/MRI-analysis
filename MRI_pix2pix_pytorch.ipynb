{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MRI_pix2pix_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOHNF1+223yZlpmEEJFlpEu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lhyochan7/MRI-analysis/blob/main/MRI_pix2pix_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiXL8Z-txheT",
        "outputId": "43541ce0-b0ea-43e8-cf5d-e5d27b72dc7d"
      },
      "source": [
        "!pip install patchify\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import nibabel as nib\n",
        "from patchify import patchify, unpatchify\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "import os\n",
        "import time\n",
        "import glob\n",
        "import numpy as np\n",
        "from numpy import savez_compressed\n",
        "from numpy import load\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: patchify in /usr/local/lib/python3.7/dist-packages (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.7/dist-packages (from patchify) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U35V7HXZbgXn",
        "outputId": "78c89b56-ec92-4f4f-e516-3030de3dab0d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM49dreTeKZS"
      },
      "source": [
        "# Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6kI01vhxnuZ"
      },
      "source": [
        "class DataLoader():\n",
        "\n",
        "  def __init__(self, o_path, t_path):\n",
        "\n",
        "    self.o_dataset = []\n",
        "    self.t_dataset = []\n",
        "    self.org_list = []\n",
        "    self.tar_list = []\n",
        "    self.o_path = o_path\n",
        "    self.t_path = t_path\n",
        "    self.filename = 'dataset.npz'\n",
        "\n",
        "  def preprocess(self):\n",
        "    \n",
        "    fig = plt.figure(figsize=(10,30))\n",
        "\n",
        "    for org_file in glob.glob(self.o_path):\n",
        "      org_base = os.path.basename(org_file)\n",
        "      org_base = org_base[0:15]\n",
        "      for tar_file in glob.glob(self.t_path):\n",
        "          tar_base = os.path.basename(tar_file)\n",
        "          tar_base = tar_base[0:15]\n",
        "\n",
        "          if tar_base == org_base:\n",
        "            org_img = nib.load(org_file)\n",
        "            tar_img = nib.load(tar_file)\n",
        "\n",
        "            # convert nii file to numpy array\n",
        "            data1 = org_img.get_fdata()\n",
        "            data2 = tar_img.get_fdata()\n",
        "\n",
        "            self.org_list.append(data1)\n",
        "            self.tar_list.append(data2)\n",
        "\n",
        "    num = 1\n",
        "\n",
        "    # patchify and convert to tensor\n",
        "    for o_data in self.org_list:\n",
        "      src_patches = patchify(o_data, (64,64,64), step = 32)\n",
        "      \n",
        "      #print(\"Number of Patches = \",len(src_patches))\n",
        "      #print(\"Shape of patch = \", src_patches.shape)\n",
        "      '''\n",
        "      ax1 = fig.add_subplot(len(self.org_list),2,num)\n",
        "      ax1.set_title('Base-Level')\n",
        "      ax1.imshow(src_patches[1,2,3,:,:,40])\n",
        "      num += 2\n",
        "      '''\n",
        "      # combine vectors 4 x 5 x 4 to 80 patches\n",
        "      input_img = np.reshape(src_patches, (-1, src_patches.shape[3], src_patches.shape[4], src_patches.shape[5], 1)) # n_patches, x, y, z\n",
        "      input_img = torch.from_numpy(input_img)\n",
        "      self.o_dataset.append(input_img)\n",
        "\n",
        "    num = 2\n",
        "\n",
        "    # patchify and convert to tensor\n",
        "    for t_data in self.tar_list:\n",
        "      target_patches = patchify(t_data, (64,64,64), step = 32)  \n",
        "      \n",
        "      '''\n",
        "      ax2 = fig.add_subplot(len(self.tar_list),2,num)\n",
        "      ax2.set_title('Target')\n",
        "      ax2.imshow(target_patches[1,2,3,:,:,40])\n",
        "      num += 2\n",
        "      '''\n",
        "      \n",
        "      # combine vectors 4 x 5 x 4 to 80 patches\n",
        "      input_img = np.reshape(src_patches, (-1, src_patches.shape[3], src_patches.shape[4], src_patches.shape[5], 1)) # n_patches, x, y, z\n",
        "      input_img = torch.from_numpy(input_img)\n",
        "      self.t_dataset.append(input_img)\n",
        "\n",
        "    # return original and target dataset\n",
        "    return self.o_dataset, self.t_dataset\n",
        "\n",
        "\n",
        "  def compress(self, a, b):\n",
        "    savez_compressed(self.filename, a, b)\n",
        "\n",
        "\n",
        "  # load and prepare training images\n",
        "  def load_real_samples(self):\n",
        "    # load compressed arrays\n",
        "    data = load(self.filename)\n",
        "    # unpack arrays\n",
        "    X1, X2 = data['arr_0'], data['arr_1']\n",
        "    return [X1, X2]\n"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxTr4Fq3ejAe"
      },
      "source": [
        "# U-Net\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "B3scD_gGySQ7",
        "outputId": "d8b619b6-467f-4e6a-f2c4-d014643ad033"
      },
      "source": [
        "# UNet\n",
        "class UNetDown(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = [nn.Conv3d(in_channels, out_channels, (4,4,4),stride=(2,2,2),padding=1,bias=False)]\n",
        "\n",
        "        if normalize:\n",
        "            layers.append(nn.InstanceNorm3d(out_channels)),\n",
        "\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        self.down = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.down(x)\n",
        "        print(\"UNetDown:\", x.shape)\n",
        "        return x\n",
        "\n",
        "'''\n",
        "# check\n",
        "x = torch.randn(1,512,4,4,4,device=device)\n",
        "model = UNetDown(512,512).to(device)\n",
        "down_out = model(x)\n",
        "print(down_out.shape)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# check\\nx = torch.randn(1,512,4,4,4,device=device)\\nmodel = UNetDown(512,512).to(device)\\ndown_out = model(x)\\nprint(down_out.shape)'"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "l8R9pRXVehsb",
        "outputId": "e197c229-8080-4dfa-e4a4-220116385e8a"
      },
      "source": [
        "class UNetUp(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = [\n",
        "            nn.ConvTranspose3d(in_channels, out_channels,(4,4,4),(2,2,2),padding=1,bias=False),\n",
        "            nn.InstanceNorm3d(out_channels),\n",
        "            nn.LeakyReLU()\n",
        "        ]\n",
        "\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        self.up = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self,x,skip):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat((x,skip),1)\n",
        "        print(\"UNetUp\", x.shape)\n",
        "        return x\n",
        "'''\n",
        "# check\n",
        "x = torch.randn(1, 32,16,16,16, device=device)\n",
        "model = UNetUp(32, 1).to(device)\n",
        "out = model(x,down_out)\n",
        "print(out.shape)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# check\\nx = torch.randn(1, 32,16,16,16, device=device)\\nmodel = UNetUp(32, 1).to(device)\\nout = model(x,down_out)\\nprint(out.shape)'"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxmQXQ3kffKw"
      },
      "source": [
        "# Generator Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvSXdVqxfgU8",
        "outputId": "38c4d495-8f07-4b83-868a-8b8906e03670"
      },
      "source": [
        "# generator: 가짜 이미지를 생성합니다.\n",
        "class GeneratorUNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.down1 = UNetDown(in_channels, 64, normalize=False)\n",
        "        self.down2 = UNetDown(64,128)                 \n",
        "        self.down3 = UNetDown(128,256)               \n",
        "        self.down4 = UNetDown(256,512,dropout=0.5) \n",
        "        self.down5 = UNetDown(512,512,normalize=False,dropout=0.5)      \n",
        "        #self.down6 = UNetDown(512,512,dropout=0.5)             \n",
        "        #self.down7 = UNetDown(512,512,dropout=0.5)              \n",
        "        #self.down8 = UNetDown(512,512,normalize=False,dropout=0.5)\n",
        "\n",
        "        self.up1 = UNetUp(512,512,dropout=0.5)\n",
        "        self.up2 = UNetUp(1024,256)\n",
        "        self.up3 = UNetUp(512,128)\n",
        "        self.up4 = UNetUp(256,64)\n",
        "        self.up5 = nn.Sequential(\n",
        "            nn.ConvTranspose3d(128,1,(4,4,4),stride=2,padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        \n",
        "        '''\n",
        "        self.up1 = UNetUp(512,512,dropout=0.5)\n",
        "        self.up2 = UNetUp(1024,512,dropout=0.5)\n",
        "        self.up3 = UNetUp(1024,512,dropout=0.5)\n",
        "        self.up4 = UNetUp(1024,512,dropout=0.5)\n",
        "        self.up5 = UNetUp(1024,256)\n",
        "        self.up6 = UNetUp(512,128)\n",
        "        self.up7 = UNetUp(256,64)\n",
        "        self.up8 = nn.Sequential(\n",
        "            nn.ConvTranspose3d(128,1,1,stride=2,padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        '''\n",
        "\n",
        "    def forward(self, x):\n",
        "        d1 = self.down1(x)\n",
        "        d2 = self.down2(d1)\n",
        "        d3 = self.down3(d2)\n",
        "        d4 = self.down4(d3)\n",
        "        d5 = self.down5(d4)\n",
        "        #d6 = self.down6(d5)\n",
        "        #d7 = self.down7(d6)\n",
        "        #d8 = self.down8(d7)\n",
        "\n",
        "\n",
        "        u1 = self.up1(d5,d4)\n",
        "        u2 = self.up2(u1,d3)\n",
        "        u3 = self.up3(u2,d2)\n",
        "        u4 = self.up4(u3,d1)\n",
        "        #u5 = self.up(u4,d2)\n",
        "        #u6 = self.up(u5,d1)\n",
        "        u5 = self.up5(u4)\n",
        "\n",
        "\n",
        "        #u1 = self.up1(d8,d7)\n",
        "        #u2 = self.up2(u1,d6)\n",
        "        #u3 = self.up3(u2,d5)\n",
        "        #u4 = self.up4(u3,d4)\n",
        "        #u5 = self.up5(u4,d3)\n",
        "        #u6 = self.up6(u5,d2)\n",
        "        #u7 = self.up7(u6,d1)\n",
        "        #u8 = self.up8(u7)\n",
        "\n",
        "        return u5\n",
        "\n",
        "# check\n",
        "x = torch.randn(1,1,64,64,64,device=device)\n",
        "model = GeneratorUNet().to(device)\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UNetDown: torch.Size([1, 64, 32, 32, 32])\n",
            "UNetDown: torch.Size([1, 128, 16, 16, 16])\n",
            "UNetDown: torch.Size([1, 256, 8, 8, 8])\n",
            "UNetDown: torch.Size([1, 512, 4, 4, 4])\n",
            "UNetDown: torch.Size([1, 512, 2, 2, 2])\n",
            "UNetUp torch.Size([1, 1024, 4, 4, 4])\n",
            "UNetUp torch.Size([1, 512, 8, 8, 8])\n",
            "UNetUp torch.Size([1, 256, 16, 16, 16])\n",
            "UNetUp torch.Size([1, 128, 32, 32, 32])\n",
            "torch.Size([1, 1, 64, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AqPq37YUYKF"
      },
      "source": [
        "# Discriminator Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qKHfML9Ua8V",
        "outputId": "f13e91b3-8a9b-448a-a0aa-536179f3afeb"
      },
      "source": [
        "class Dis_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, normalize=True):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = [nn.Conv3d(in_channels, out_channels, (3,3,3), (2,2,2), padding=1)]\n",
        "        if normalize:\n",
        "            layers.append(nn.InstanceNorm3d(out_channels))\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "    \n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block(x)\n",
        "        print(x.shape)\n",
        "        return x\n",
        "\n",
        "# check\n",
        "x = torch.randn(1,1,64,64,64,device=device)\n",
        "model = Dis_block(1,64).to(device)\n",
        "out = model(x)\n",
        "print(out.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64, 32, 32, 32])\n",
            "torch.Size([1, 64, 32, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXwQXrNoUTwr",
        "outputId": "759ba9ba-d6b1-4540-bc53-f65d1de3cb12"
      },
      "source": [
        "# Discriminator은 patch gan을 사용합니다.\n",
        "# Patch Gan: 이미지를 16x16의 패치로 분할하여 각 패치가 진짜인지 가짜인지 식별합니다.\n",
        "# high-frequency에서 정확도가 향상됩니다.\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.stage_1 = Dis_block(in_channels*2,64,normalize=False)\n",
        "        self.stage_2 = Dis_block(64,128)\n",
        "        self.stage_3 = Dis_block(128,256)\n",
        "        self.stage_4 = Dis_block(256,512)\n",
        "\n",
        "        self.patch = nn.Conv3d(512,1,3,padding=1) # 16x16 패치 생성\n",
        "\n",
        "    def forward(self,a,b):\n",
        "        x = torch.cat((a,b),1)\n",
        "        x = self.stage_1(x)\n",
        "        x = self.stage_2(x)\n",
        "        x = self.stage_3(x)\n",
        "        x = self.stage_4(x)\n",
        "        x = self.patch(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n",
        "# check\n",
        "x = torch.randn(1,1,64,64,64,device=device)\n",
        "model = Discriminator().to(device)\n",
        "out = model(x,x)\n",
        "print(out.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64, 32, 32, 32])\n",
            "torch.Size([1, 128, 16, 16, 16])\n",
            "torch.Size([1, 256, 8, 8, 8])\n",
            "torch.Size([1, 512, 4, 4, 4])\n",
            "torch.Size([1, 1, 4, 4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WduidCZdXtkZ"
      },
      "source": [
        "# Initalize Weights of Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcCCLw3gXhdH"
      },
      "source": [
        "model_gen = GeneratorUNet().to(device)\n",
        "model_dis = Discriminator().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1-bQhpYXkMN"
      },
      "source": [
        "# 가중치 초기화\n",
        "def initialize_weights(model):\n",
        "    class_name = model.__class__.__name__\n",
        "    if class_name.find('Conv') != -1:\n",
        "        nn.init.normal_(model.weight.data, 0.0, 0.02)\n",
        "\n",
        "\n",
        "# 가중치 초기화 적용\n",
        "model_gen.apply(initialize_weights);\n",
        "model_dis.apply(initialize_weights);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46HzbJmHXlZw",
        "outputId": "e328176e-6d0e-442c-f47c-8bd6b55853af"
      },
      "source": [
        "model_gen.eval()\n",
        "model_dis.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (stage_1): Dis_block(\n",
              "    (block): Sequential(\n",
              "      (0): Conv3d(2, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
              "      (1): LeakyReLU(negative_slope=0.2)\n",
              "    )\n",
              "  )\n",
              "  (stage_2): Dis_block(\n",
              "    (block): Sequential(\n",
              "      (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
              "      (1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "      (2): LeakyReLU(negative_slope=0.2)\n",
              "    )\n",
              "  )\n",
              "  (stage_3): Dis_block(\n",
              "    (block): Sequential(\n",
              "      (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
              "      (1): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "      (2): LeakyReLU(negative_slope=0.2)\n",
              "    )\n",
              "  )\n",
              "  (stage_4): Dis_block(\n",
              "    (block): Sequential(\n",
              "      (0): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))\n",
              "      (1): InstanceNorm3d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "      (2): LeakyReLU(negative_slope=0.2)\n",
              "    )\n",
              "  )\n",
              "  (patch): Conv3d(512, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFeZVqTxa2JV"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zztNlTvBa4yP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "991d43c6-af4b-40a5-8cb7-4112d167801a"
      },
      "source": [
        "# Preprocess MRI images\n",
        "data = DataLoader('/content/drive/MyDrive/Data/Original/*.gz', '/content/drive/MyDrive/Data/Target/*.gz')\n",
        "a, b = data.preprocess()\n",
        "\n",
        "print(len(a))\n",
        "print(len(b))\n",
        "\n",
        "print(a[0].shape)\n",
        "\n",
        "# a = original, b = target image\n",
        "data.compress(a,b)\n",
        "dataset = data.load_real_samples()"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "10\n",
            "torch.Size([80, 64, 64, 64, 1])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-197-f7f3e24845e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# a = original, b = target image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_real_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-196-9d4fa462ba70>\u001b[0m in \u001b[0;36mcompress\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0msavez_compressed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msavez_compressed\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavez_compressed\u001b[0;34m(file, *args, **kwds)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m     \"\"\"\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0m_savez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m_savez\u001b[0;34m(file, args, kwds, compress, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnamedict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;31m# always force zip64, gh-10776\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mzipf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_zip64\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \"\"\"\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x2160 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v49wJunJXNIB"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Om7IyXiXLmg"
      },
      "source": [
        "# 손실함수\n",
        "loss_func_gan = nn.BCELoss()\n",
        "loss_func_pix = nn.L1Loss()\n",
        "\n",
        "# loss_func_pix 가중치\n",
        "lambda_pixel = 100\n",
        "\n",
        "# patch 수\n",
        "patch = (1,4,4,4)\n",
        "\n",
        "# 최적화 파라미터\n",
        "from torch import optim\n",
        "lr = 2e-4\n",
        "beta1 = 0.5\n",
        "beta2 = 0.999\n",
        "\n",
        "opt_dis = optim.Adam(model_dis.parameters(),lr=lr,betas=(beta1,beta2))\n",
        "opt_gen = optim.Adam(model_gen.parameters(),lr=lr,betas=(beta1,beta2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "mS6VTj3dXTmB",
        "outputId": "9a98654c-86a9-4190-fea2-333f4921c508"
      },
      "source": [
        "# 학습\n",
        "model_gen.train()\n",
        "model_dis.train()\n",
        "\n",
        "batch_count = 0\n",
        "num_epochs = 100\n",
        "start_time = time.time()\n",
        "\n",
        "loss_hist = {'gen':[],\n",
        "             'dis':[]}\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for a, b in dataset:\n",
        "        ba_si = a.size(0)\n",
        "\n",
        "        # real image\n",
        "        real_a = a.to(device)\n",
        "        real_b = b.to(device)\n",
        "\n",
        "        # patch label\n",
        "        real_label = torch.ones(ba_si, *patch, requires_grad=False).to(device)\n",
        "        fake_label = torch.zeros(ba_si, *patch, requires_grad=False).to(device)\n",
        "\n",
        "        # generator\n",
        "        model_gen.zero_grad()\n",
        "\n",
        "        fake_b = model_gen(real_a) # 가짜 이미지 생성\n",
        "        out_dis = model_dis(fake_b, real_b) # 가짜 이미지 식별\n",
        "\n",
        "        gen_loss = loss_func_gan(out_dis, real_label)\n",
        "        pixel_loss = loss_func_pix(fake_b, real_b)\n",
        "\n",
        "        g_loss = gen_loss + lambda_pixel * pixel_loss\n",
        "        g_loss.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        # discriminator\n",
        "        model_dis.zero_grad()\n",
        "\n",
        "        out_dis = model_dis(real_b, real_a) # 진짜 이미지 식별\n",
        "        real_loss = loss_func_gan(out_dis,real_label)\n",
        "        \n",
        "        out_dis = model_dis(fake_b.detach(), real_a) # 가짜 이미지 식별\n",
        "        fake_loss = loss_func_gan(out_dis,fake_label)\n",
        "\n",
        "        d_loss = (real_loss + fake_loss) / 2.\n",
        "        d_loss.backward()\n",
        "        opt_dis.step()\n",
        "\n",
        "        loss_hist['gen'].append(g_loss.item())\n",
        "        loss_hist['dis'].append(d_loss.item())\n",
        "\n",
        "        batch_count += 1\n",
        "        if batch_count % 100 == 0:\n",
        "            print('Epoch: %.0f, G_Loss: %.6f, D_Loss: %.6f, time: %.2f min' %(epoch, g_loss.item(), d_loss.item(), (time.time()-start_time)/60))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-178-83de18f5a39d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mba_si\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dl' is not defined"
          ]
        }
      ]
    }
  ]
}