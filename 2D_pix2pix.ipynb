{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2D_pix2pix.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOnXCxLkEbTjJZ2EQZc/jK0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lhyochan7/MRI-analysis/blob/main/2D_pix2pix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKyP9G6UR_4w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8ece21e-8b9d-45e0-f292-d36786507d87"
      },
      "source": [
        "# Mount your Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytaDWSZuxfVq",
        "outputId": "a77cae3c-5390-4d57-e9f0-03ee198388bb"
      },
      "source": [
        "pip install pydicom"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydicom\n",
            "  Downloading pydicom-2.2.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 8.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dplSztuSUIL"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import nibabel as nib\n",
        "from matplotlib import pyplot as plt\n",
        "import pydicom\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "import os\n",
        "import time\n",
        "import glob\n",
        "import numpy as np\n",
        "from numpy import savez_compressed\n",
        "from numpy import load\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fMc2KEKY2dR"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize([0.5],[0.5]),\n",
        "                    transforms.Resize((256,256))\n",
        "])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1R1wJb7SWgy"
      },
      "source": [
        "class DataLoader():\n",
        "  def __init__(self, o_path, t_path):\n",
        "    self.o_dataset = []\n",
        "    self.t_dataset = []\n",
        "    self.org_list = []\n",
        "    self.tar_list = []\n",
        "    self.o_path = o_path\n",
        "    self.t_path = t_path\n",
        "\n",
        "  def preprocess(self):\n",
        "    for org_file in glob.glob(self.o_path):\n",
        "      org_base = os.path.basename(org_file)\n",
        "      org_base = org_base[0:15]\n",
        "      for tar_file in glob.glob(self.t_path):\n",
        "        tar_base = os.path.basename(tar_file)\n",
        "        tar_base = tar_base[0:15]\n",
        "        if tar_base[0:9] == org_base[0:9]:\n",
        "          org_img = pydicom.dcmread(org_file)\n",
        "          tar_img = pydicom.dcmread(tar_file)\n",
        "\n",
        "          # convert dicom file to numpy array\n",
        "          data1 = org_img.pixel_array\n",
        "          data2 = tar_img.pixel_array\n",
        "\n",
        "\n",
        "          data1 = data1.astype('float32')\n",
        "          data2 = data2.astype('float32')\n",
        "\n",
        "          data1 = transform(data1)\n",
        "          data2 = transform(data2)\n",
        "\n",
        "          data1 = np.reshape(data1, (1,1,256,256))\n",
        "          data2 = np.reshape(data2, (1,1,256,256))\n",
        "          \n",
        "          self.org_list.append(data1)\n",
        "          self.tar_list.append(data2)\n",
        "    \n",
        "    org_output = self.org_list[0]\n",
        "    tar_output = self.tar_list[0]\n",
        "\n",
        "    count = 0\n",
        "    for o_data in self.org_list:\n",
        "      if count == 0:\n",
        "        count += 1\n",
        "        continue\n",
        "      org_output = torch.cat([org_output, o_data], 0)\n",
        "    \n",
        "    self.o_dataset.append(org_output)\n",
        "\n",
        "    count = 0\n",
        "    for t_data in self.tar_list:\n",
        "      if count == 0:\n",
        "        count += 1\n",
        "        continue\n",
        "      tar_output = torch.cat([tar_output, t_data], 0)\n",
        "\n",
        "    self.t_dataset.append(tar_output)\n",
        "\n",
        "    # return original and target dataset\n",
        "    print(self.o_dataset[0].shape)\n",
        "    return self.o_dataset, self.t_dataset"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOiiU4acWqf_",
        "outputId": "62853586-ae48-4bf0-880c-6ba63de698cb"
      },
      "source": [
        "data = DataLoader('/content/drive/MyDrive/GAN_Data/ADNI_002_S_1070_2006_result/*.dcm', '/content/drive/MyDrive/GAN_Data/ADNI_002_S_1070_2009_result/*.dcm')\n",
        "orig, target = data.preprocess()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([160, 1, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PROu4niRb7gQ"
      },
      "source": [
        "# UNet\n",
        "class UNetDown(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = [nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1, bias=False)]\n",
        "\n",
        "        if normalize:\n",
        "            layers.append(nn.InstanceNorm2d(out_channels)),\n",
        "\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        self.down = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.down(x)\n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAluLp3Ee_SB"
      },
      "source": [
        "class UNetUp(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(in_channels, out_channels,4,2,1,bias=False),\n",
        "            nn.InstanceNorm2d(out_channels),\n",
        "            nn.LeakyReLU()\n",
        "        ]\n",
        "\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        self.up = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self,x,skip):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat((x,skip),1)\n",
        "        return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wvFnCh6ge_z4",
        "outputId": "044e5241-0faf-4664-ddd5-253ea0377139"
      },
      "source": [
        "# generator: 가짜 이미지를 생성합니다.\n",
        "class GeneratorUNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.down1 = UNetDown(in_channels, 64, normalize=False)\n",
        "        self.down2 = UNetDown(64,128)                 \n",
        "        self.down3 = UNetDown(128,256)               \n",
        "        self.down4 = UNetDown(256,512,dropout=0.5) \n",
        "        self.down5 = UNetDown(512,512,dropout=0.5)      \n",
        "        self.down6 = UNetDown(512,512,dropout=0.5)             \n",
        "        self.down7 = UNetDown(512,512,dropout=0.5)              \n",
        "        self.down8 = UNetDown(512,512,normalize=False,dropout=0.5)\n",
        "\n",
        "        self.up1 = UNetUp(512,512,dropout=0.5)\n",
        "        self.up2 = UNetUp(1024,512,dropout=0.5)\n",
        "        self.up3 = UNetUp(1024,512,dropout=0.5)\n",
        "        self.up4 = UNetUp(1024,512,dropout=0.5)\n",
        "        self.up5 = UNetUp(1024,256)\n",
        "        self.up6 = UNetUp(512,128)\n",
        "        self.up7 = UNetUp(256,64)\n",
        "        self.up8 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128,1,4,stride=2,padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        d1 = self.down1(x)\n",
        "        d2 = self.down2(d1)\n",
        "        d3 = self.down3(d2)\n",
        "        d4 = self.down4(d3)\n",
        "        d5 = self.down5(d4)\n",
        "        d6 = self.down6(d5)\n",
        "        d7 = self.down7(d6)\n",
        "        d8 = self.down8(d7)\n",
        "\n",
        "        u1 = self.up1(d8,d7)\n",
        "        u2 = self.up2(u1,d6)\n",
        "        u3 = self.up3(u2,d5)\n",
        "        u4 = self.up4(u3,d4)\n",
        "        u5 = self.up5(u4,d3)\n",
        "        u6 = self.up6(u5,d2)\n",
        "        u7 = self.up7(u6,d1)\n",
        "        u8 = self.up8(u7)\n",
        "\n",
        "        return u8\n",
        "'''\n",
        "# check\n",
        "x = torch.randn(160,1,256,256,device=device)\n",
        "model = GeneratorUNet().to(device)\n",
        "out = model(x)\n",
        "print(out.shape)'''"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# check\\nx = torch.randn(160,1,256,256,device=device)\\nmodel = GeneratorUNet().to(device)\\nout = model(x)\\nprint(out.shape)'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrxKnOrJfCvI"
      },
      "source": [
        "class Dis_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, normalize=True):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = [nn.Conv2d(in_channels, out_channels, 3, stride=2, padding=1)]\n",
        "        if normalize:\n",
        "            layers.append(nn.InstanceNorm2d(out_channels))\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "    \n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block(x)\n",
        "        return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHhCEMTzfFNP"
      },
      "source": [
        "# Discriminator은 patch gan을 사용합니다.\n",
        "# Patch Gan: 이미지를 16x16의 패치로 분할하여 각 패치가 진짜인지 가짜인지 식별합니다.\n",
        "# high-frequency에서 정확도가 향상됩니다.\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.stage_1 = Dis_block(in_channels*2,64,normalize=False)\n",
        "        self.stage_2 = Dis_block(64,128)\n",
        "        self.stage_3 = Dis_block(128,256)\n",
        "        self.stage_4 = Dis_block(256,512)\n",
        "\n",
        "        self.patch = nn.Conv2d(512,1,1,padding=1) # 16x16 패치 생성\n",
        "\n",
        "    def forward(self,a,b):\n",
        "        x = torch.cat((a,b),1)\n",
        "        x = self.stage_1(x)\n",
        "        x = self.stage_2(x)\n",
        "        x = self.stage_3(x)\n",
        "        x = self.stage_4(x)\n",
        "        x = self.patch(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJGNC8yCfJyn"
      },
      "source": [
        "model_gen = GeneratorUNet().to(device)\n",
        "model_dis = Discriminator().to(device)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e03DRW9GfMsf"
      },
      "source": [
        "# 가중치 초기화\n",
        "def initialize_weights(model):\n",
        "    class_name = model.__class__.__name__\n",
        "    if class_name.find('Conv') != -1:\n",
        "        nn.init.normal_(model.weight.data, 0.0, 0.02)\n",
        "\n",
        "\n",
        "# 가중치 초기화 적용\n",
        "model_gen.apply(initialize_weights);\n",
        "model_dis.apply(initialize_weights);"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HfHbcBefOlv"
      },
      "source": [
        "# 손실함수\n",
        "loss_func_gan = nn.BCELoss()\n",
        "loss_func_pix = nn.L1Loss()\n",
        "\n",
        "# loss_func_pix 가중치\n",
        "lambda_pixel = 100\n",
        "\n",
        "# patch 수\n",
        "patch = (1,256//2**4,256//2**4)\n",
        "\n",
        "# 최적화 파라미터\n",
        "from torch import optim\n",
        "lr = 2e-4\n",
        "beta1 = 0.5\n",
        "beta2 = 0.999\n",
        "\n",
        "opt_dis = optim.Adam(model_dis.parameters(),lr=lr,betas=(beta1,beta2))\n",
        "opt_gen = optim.Adam(model_gen.parameters(),lr=lr,betas=(beta1,beta2))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVTvu6B6fRMf",
        "outputId": "4d69379e-893e-4d55-c3ed-fb5b1eaf7963"
      },
      "source": [
        "# 학습\n",
        "model_gen.train()\n",
        "model_dis.train()\n",
        "\n",
        "batch_count = 0\n",
        "num_epochs = 100\n",
        "start_time = time.time()\n",
        "\n",
        "loss_hist = {'gen':[],\n",
        "             'dis':[]}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for a, b in zip(orig, target):\n",
        "        ba_si = a.size(0)\n",
        "\n",
        "        # real image\n",
        "        real_a = a.to(device)\n",
        "        real_b = b.to(device)\n",
        "\n",
        "        # patch label\n",
        "        real_label = torch.ones(ba_si, *patch, requires_grad=False).to(device)\n",
        "        fake_label = torch.zeros(ba_si, *patch, requires_grad=False).to(device)\n",
        "\n",
        "        # generator\n",
        "        model_gen.zero_grad()\n",
        "\n",
        "        fake_b = model_gen(real_a) # 가짜 이미지 생성\n",
        "        out_dis = model_dis(fake_b, real_b) # 가짜 이미지 식별\n",
        "\n",
        "        gen_loss = loss_func_gan(out_dis, real_label)\n",
        "        pixel_loss = loss_func_pix(fake_b, real_b)\n",
        "\n",
        "        g_loss = gen_loss + lambda_pixel * pixel_loss\n",
        "        g_loss.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        # discriminator\n",
        "        model_dis.zero_grad()\n",
        "\n",
        "        out_dis = model_dis(real_b, real_a) # 진짜 이미지 식별\n",
        "        real_loss = loss_func_gan(out_dis,real_label)\n",
        "        \n",
        "        out_dis = model_dis(fake_b.detach(), real_a) # 가짜 이미지 식별\n",
        "        fake_loss = loss_func_gan(out_dis,fake_label)\n",
        "\n",
        "        d_loss = (real_loss + fake_loss) / 2.\n",
        "        d_loss.backward()\n",
        "        opt_dis.step()\n",
        "\n",
        "        loss_hist['gen'].append(g_loss.item())\n",
        "        loss_hist['dis'].append(d_loss.item())\n",
        "\n",
        "        batch_count += 1\n",
        "        if batch_count % 1 == 0:\n",
        "            print('Epoch: %.0f, G_Loss: %.6f, D_Loss: %.6f, time: %.2f min' %(epoch, g_loss.item(), d_loss.item(), (time.time()-start_time)/60))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([160, 64, 128, 128])\n",
            "torch.Size([160, 128, 64, 64])\n",
            "torch.Size([160, 256, 32, 32])\n",
            "torch.Size([160, 512, 16, 16])\n",
            "torch.Size([160, 512, 8, 8])\n",
            "torch.Size([160, 512, 4, 4])\n",
            "torch.Size([160, 512, 2, 2])\n",
            "torch.Size([160, 512, 1, 1])\n",
            "torch.Size([160, 512, 2, 2])\n",
            "torch.Size([160, 512, 4, 4])\n",
            "torch.Size([160, 512, 8, 8])\n",
            "torch.Size([160, 512, 16, 16])\n",
            "torch.Size([160, 256, 32, 32])\n",
            "torch.Size([160, 128, 64, 64])\n",
            "torch.Size([160, 64, 128, 128])\n"
          ]
        }
      ]
    }
  ]
}